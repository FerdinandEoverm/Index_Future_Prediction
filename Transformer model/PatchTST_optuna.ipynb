{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db31c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.chdir('d:/future/Index_Future_Prediction')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import optuna\n",
    "import math\n",
    "from datetime import datetime\n",
    "import backtrader as bt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler, Adam, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from utils import *\n",
    "from modules import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970bfe8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch_TST(nn.Module):\n",
    "    \"\"\"Patch Time Series Transformer\"\"\"\n",
    "    def __init__(self, input_size, seq_len, patch_size, stride, num_layer, num_head, d_model, masking_ratio, mask_expand_size, dropout_1, dropout_2, dropout_3):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda:0'\n",
    "        self.input_size = input_size\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.masking_ratio = masking_ratio\n",
    "        self.mask_expand_size = mask_expand_size\n",
    "\n",
    "        self.num_patch = int(np.floor((seq_len - patch_size) / stride) + 1)\n",
    "\n",
    "        self.patch = TimeSeriesPatcher(patch_size, stride) # 首先经过patcher分成子序列\n",
    "\n",
    "        self.projection = PatchProjection(input_size, patch_size, d_model = d_model, dropout = dropout_1)\n",
    "\n",
    "        self.encoder = MultiLayerEncoder(dim_feature = d_model, dim_sequence = self.num_patch, num_enc_layer = num_layer, num_head = num_head, num_ffn_hidden = d_model*2, dropout = dropout_2)\n",
    "\n",
    "        self.reconstruction = nn.Linear(d_model, input_size * patch_size)\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Flatten(start_dim = -2),\n",
    "            nn.Linear(self.num_patch * d_model, self.num_patch * d_model),\n",
    "            nn.Dropout(dropout_3),\n",
    "            HybridDecoder(dim_state = self.num_patch * d_model, init_prob = [0.0,0.0,0.0])\n",
    "        )\n",
    "    \n",
    "    def self_supervised(self, x):\n",
    "        \"\"\"\n",
    "        自监督预训练\n",
    "        如果不允许patch重叠，正好被patch隔断的形态无法学习到。\n",
    "        允许patch重叠，则模型预训练的时候可以从前后patch偷看到信息。\n",
    "        需要用双重mask\n",
    "        target mask 是真正需要重建的patch\n",
    "        input mask 是target mask的扩展，根据系数向两侧扩展掩蔽范围。例如假如patch允许重叠50%，则向前后各多屏蔽一个patch就可以完全屏蔽掉信息。\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "\n",
    "        noise = torch.rand(size=(batch_size, self.num_patch), device=device)\n",
    "        target_mask = noise < self.masking_ratio\n",
    "        \n",
    "        # 防止出现所有 patch 都没被 mask 的情况，至少 mask 一个随机选择一个 patch 进行 mask\n",
    "        if not target_mask.any(dim=1).all():\n",
    "            for i in range(batch_size):\n",
    "                if not target_mask[i].any():\n",
    "                    fallback_idx = torch.randint(0, self.num_patch, (1,)).item()\n",
    "                    target_mask[i, fallback_idx] = True\n",
    "\n",
    "        target_mask_float = target_mask.float().unsqueeze(1)\n",
    "        kernel_size = 2 * self.mask_expand_size + 1\n",
    "        kernel = torch.ones(1, 1, kernel_size, device=device)\n",
    "        padding = self.mask_expand_size\n",
    "        expanded_mask_float = F.conv1d(target_mask_float, kernel, padding=padding)\n",
    "        input_mask = (expanded_mask_float > 0).squeeze(1)\n",
    "\n",
    "\n",
    "        x_patched = self.patch(x)\n",
    "        reshape_mask = input_mask.unsqueeze(-1)\n",
    "        x_masked = torch.where(reshape_mask, 0.0, x_patched)\n",
    "        x_projected = self.projection(x_masked)\n",
    "        x_encodered = self.encoder(x_projected)\n",
    "        \n",
    "        x_pre_reconstruction = x_encodered[target_mask] # 仅关注target mask\n",
    "        x_reconstructed = self.reconstruction(x_pre_reconstruction)\n",
    "        x_target = x_patched[target_mask] # 仅关注target mask\n",
    "\n",
    "        return x_reconstructed, x_target\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播输出\"\"\"\n",
    "        x_patched = self.patch(x)\n",
    "        x_projected = self.projection(x_patched)\n",
    "        x_encodered = self.encoder(x_projected)\n",
    "        output = self.output(x_encodered)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef94da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 需要调优的超参数\n",
    "    seq_len = trial.suggest_int(\"seq_len\", 60, 250)\n",
    "    patch_size = trial.suggest_int(\"patch_size\", 5, 30)\n",
    "    num_layer = trial.suggest_categorical('num_layer', [1,2,3,4,5])\n",
    "    num_head = trial.suggest_categorical('num_head', [4,8,16])\n",
    "    d_model = trial.suggest_categorical('d_model', [32, 64, 128, 256])\n",
    "    \n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "\n",
    "    dropout_1 = trial.suggest_float(\"dropout_1\", 0.0, 0.5)\n",
    "    dropout_2 = trial.suggest_float(\"dropout_2\", 0.0, 0.5)\n",
    "    dropout_3 = trial.suggest_float(\"dropout_3\", 0.0, 0.5)\n",
    "\n",
    "    learning_rate_encoder = trial.suggest_float(\"learning_rate_encoder\", 1e-6, 1e-2, log=True)\n",
    "    weight_decay_encoder = trial.suggest_float(\"weight_decay_encoder\", 1e-7, 1e-3, log=True)\n",
    "    learning_rate_output = trial.suggest_float(\"learning_rate_output\", 1e-6, 1e-2, log=True)\n",
    "    weight_decay_output = trial.suggest_float(\"weight_decay_output\", 1e-7, 1e-3, log=True)\n",
    "\n",
    "    alpha = trial.suggest_float(\"alpha\", 1e-2, 1e-1, log=True)\n",
    "    delta = trial.suggest_float(\"delta\", 1.0, 1.3)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.7, 1, log=True)\n",
    "\n",
    "\n",
    "    # 由选择的超参数决定\n",
    "    masking_ratio = 0.2\n",
    "    mask_expand_size = trial.suggest_categorical('mask_expand_size', [1, 2, 3])\n",
    "    stride = math.ceil(patch_size / (mask_expand_size + 1))\n",
    "\n",
    "\n",
    "    model = Patch_TST(input_size = 10,\n",
    "                    seq_len = seq_len,\n",
    "                    patch_size = patch_size,\n",
    "                    stride = stride,\n",
    "                    num_layer = num_layer, \n",
    "                    num_head = num_head,\n",
    "                    d_model = d_model,\n",
    "                    masking_ratio = masking_ratio,\n",
    "                    mask_expand_size = mask_expand_size,\n",
    "                    dropout_1 = dropout_1,\n",
    "                    dropout_2 = dropout_2,\n",
    "                    dropout_3 = dropout_3,\n",
    "                    ).to('cuda:0')\n",
    "    \n",
    "    output_params = model.output.parameters()\n",
    "    output_param_ids = {id(p) for p in output_params}\n",
    "    other_params = [p for p in model.parameters() if id(p) not in output_param_ids]\n",
    "    optimizer_grouped_parameters = [\n",
    "                {'params': other_params, 'lr': learning_rate_encoder, 'weight_decay': weight_decay_encoder},\n",
    "                {'params': model.output.parameters(), 'lr': learning_rate_output, 'weight_decay': weight_decay_output}\n",
    "            ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate_encoder, weight_decay=weight_decay_encoder)\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "\n",
    "\n",
    "    # 提取数据\n",
    "    assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX', 'AU.SHF', 'JM.DCE','RB.SHF','HC.SHF', 'I.DCE', 'M.DCE', 'CF.ZCE',]\n",
    "\n",
    "    feature_columns = ['inday_chg_open','inday_chg_high','inday_chg_low','inday_chg_close','inday_chg_amplitude', 'ma_10','ma_26','ma_45','ma_90','ma_vol',]\n",
    "    label_columns = ['label_return','down_prob','middle_prob','up_prob']\n",
    "\n",
    "    feature = []\n",
    "    label = []\n",
    "    for asset_code in assets_list:\n",
    "        data = pd.read_csv(f'data/{asset_code}.csv')\n",
    "        data = data[data['trade_date'] < 20230901].copy() # 所有2023年以后数据不参与训练\n",
    "        feature.append(torch.tensor(data[feature_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "        label.append(torch.tensor(data[label_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "\n",
    "    feature = torch.stack(feature, dim = 1)\n",
    "    label = torch.stack(label, dim = 1)\n",
    "    feature = feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(2,3)\n",
    "    label = label[seq_len-1:]\n",
    "    data = RandomLoader(feature, label)\n",
    "    train_loader, test_loader = data(batch_size=batch_size, slice_size=[0.8,0.19], balance=[True, True])\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    def pre_epoch():\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_reconstructed, x_target = model.self_supervised(batch_x)\n",
    "            loss = loss_fn(x_reconstructed, x_target)\n",
    "            train_losses.append(loss.item()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        test_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                x_reconstructed, x_target = model.self_supervised(batch_x)\n",
    "                loss = loss_fn(x_reconstructed, x_target)\n",
    "                test_losses.append(loss.item()) \n",
    "        return np.mean(train_losses), np.mean(test_losses)\n",
    "\n",
    "    def pre_train(epochs = 30):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for i in tqdm.tqdm(range(epochs)):\n",
    "            train_loss, test_loss = pre_epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            scheduler.step()\n",
    "        plt.plot(range(epochs), train_losses)\n",
    "        plt.plot(range(epochs), test_losses)\n",
    "        plt.show()\n",
    "        return np.mean(test_losses[-10:])\n",
    "\n",
    "    pre_train(30)\n",
    "\n",
    "\n",
    "    # 提取部分数据用于二阶段训练\n",
    "    assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX', 'AU.SHF', 'JM.DCE','RB.SHF','HC.SHF', 'I.DCE', 'M.DCE', 'CF.ZCE',]\n",
    "    # assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX']\n",
    "    # assets_list = ['JM.DCE','RB.SHF','HC.SHF', 'I.DCE']\n",
    "\n",
    "    feature = []\n",
    "    label = []\n",
    "    for asset_code in assets_list:\n",
    "        data = pd.read_csv(f'data/{asset_code}.csv')\n",
    "        data = data[data['trade_date'] < 20230901].copy() # 所有2023年以后数据不参与训练\n",
    "        feature.append(torch.tensor(data[feature_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "        label.append(torch.tensor(data[label_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "\n",
    "    feature = torch.stack(feature, dim = 1)\n",
    "    label = torch.stack(label, dim = 1)\n",
    "    feature = feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(2,3)\n",
    "    label = label[seq_len-1:]\n",
    "    data = RandomLoader(feature, label)\n",
    "    train_loader, test_loader = data(batch_size=batch_size, slice_size=[0.8,0.19], balance=[True, True])\n",
    "\n",
    "\n",
    "    loss_fn = HybridLoss(alpha = alpha, delta = delta, show_loss = False)\n",
    "\n",
    "    def epoch():\n",
    "        train_losses = []\n",
    "        model.train()\n",
    "        model.projection.eval()\n",
    "        model.encoder.eval()\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_x)\n",
    "            loss = loss_fn(pred, batch_y)\n",
    "            train_losses.append(loss.item()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        test_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                pred = model(batch_x)\n",
    "                loss = loss_fn(pred, batch_y)\n",
    "                test_losses.append(loss.item()) \n",
    "        return np.mean(train_losses), np.mean(test_losses)\n",
    "\n",
    "    def train(epochs = 30):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        for i in tqdm.tqdm(range(epochs)):\n",
    "            train_loss, test_loss = epoch()\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            scheduler.step()\n",
    "        plt.plot(range(epochs), train_losses)\n",
    "        plt.plot(range(epochs), test_losses)\n",
    "        plt.show()\n",
    "        return np.mean(test_losses[-3:])\n",
    "\n",
    "    final_loss = train(10)\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce52db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    study_name=\"patchtst_all\",\n",
    "    storage=\"sqlite:///data/db.sqlite3_all\",  # 保存到 SQLite 文件\n",
    "    load_if_exists=True # 如果存在同名study，则加载它\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=10)\n",
    "print(\"最佳准确率: \", study.best_value)\n",
    "print(\"最佳超参数: \", study.best_params)\n",
    "df = study.trials_dataframe()\n",
    "df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff91b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2b1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_param_importances(study)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "future",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
