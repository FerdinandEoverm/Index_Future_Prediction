{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d70f4bb",
   "metadata": {},
   "source": [
    "这里 我们使用普通RNN和SegRNN架构对比patch的作用\n",
    "\n",
    "为什么要进行Patch？\n",
    "\n",
    "我们希望收集一个长序列的信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f711495",
   "metadata": {},
   "source": [
    "我们可以将RNN类的模型理解为一个顺序的信息收集器，这个收集器可以从前向后逐步遍历所有的时间步，\n",
    "\n",
    "在每个时间步上，得到的信息储存是之前的历史信息+本次收集的信息，在最后用收集的信息进行输出\n",
    "\n",
    "因为提升预测的需要，我们必须扩展序列长度，来获取更全面的信息；\n",
    "\n",
    "但是如果总的距离过长，就必须压缩历史信息的占比，导致远距离信息微弱甚至丢失\n",
    "\n",
    "而如果不压缩历史信息，会导致梯度爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899375c0",
   "metadata": {},
   "source": [
    "因此我们可以参考CNN的思路，采用1维卷积的方式尝试解决这个问题：\n",
    "\n",
    "首先，我们将原始序列分为长度为某个值子序列，然后每若干步采样一次。\n",
    "\n",
    "在每个子序列内，可以使用Linear或RNN，尝试在序列中识别出小段的上涨趋势、下跌趋势和平盘之类的信息，并抽象为信息向量；\n",
    "\n",
    "由于每个子序列内长度有限，RNN可以充分吸取信息而不必担心长序列信息丢失的问题。\n",
    "\n",
    "如果此时的序列仍然过长，我们可以再加入一层，将上一层得到的子序列再分组为新的子序列，同样用Linear或RNN收集子序列信息。\n",
    "\n",
    "在这一层我们可以识别出更复杂的形态组合，例如连续多端上涨之后的下跌，抑或是平盘之后的变盘形态等等。\n",
    "\n",
    "直到整个序列的长度已经很小了，此时我们再使用RNN进行最后一次收集，并将的得到的信息向量传入输出层输出出我们需要的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b5989",
   "metadata": {},
   "source": [
    "这样一来，每层的RNN都面对一个相对较小的子序列，不至于出现长距离信息丢失的问题\n",
    "\n",
    "而不同层的RNN处理的问题是不一样的，其参数和方式也有所不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f92810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('d:/future/Index_Future_Prediction')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler, Adam, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6d7a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.patch import SimplePatch\n",
    "from modules.truncate import SequenceTruncate\n",
    "\n",
    "class Patch_LSTM(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, input_size, patch_size, in_patch_hidden_size, in_patch_num_layers, hidden_size, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda:0'\n",
    "        self.input_size = input_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_patch_hidden_size = in_patch_hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.sequence_truncate = SequenceTruncate(dropout)\n",
    "\n",
    "        self.simple_patch = SimplePatch(patch_size)\n",
    "\n",
    "        self.inpatch_process = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = in_patch_hidden_size,\n",
    "            num_layers = in_patch_num_layers,\n",
    "            dropout = dropout,\n",
    "            batch_first = True,\n",
    "            # nonlinearity='relu',\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.process = nn.LSTM(\n",
    "            input_size = in_patch_hidden_size, #第一层LSTM的隐藏层作为第二层的输入，因此第二层的input size = in_patch_hidden_size\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            dropout = dropout,\n",
    "            batch_first = True,\n",
    "            # nonlinearity='relu',\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            HybridDecoder(dim_state = hidden_size, init_prob = [0.0,0.5,0.0])\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_truncated = self.sequence_truncate(x)\n",
    "        x_patched = self.simple_patch(x_truncated)\n",
    "        num_patch = x_patched.shape[-3]\n",
    "        front_size = tuple(x_patched.shape[:-2])\n",
    "        x_rebatched = x_patched.reshape(-1, self.patch_size, self.input_size)\n",
    "        x_processed_1 = self.inpatch_process(x_rebatched)[0][:,-1,:]\n",
    "        x_recover = x_processed_1.reshape(*front_size, -1)\n",
    "        front_size_2 = tuple(x_recover.shape[:-2])\n",
    "        x_rebatched_2 = x_recover.reshape(-1, num_patch,  self.in_patch_hidden_size)\n",
    "        x_rebatched_2 = self.dropout(x_rebatched_2)\n",
    "        x_processed_2 = self.process(x_rebatched_2)[0][:,-1,:]\n",
    "        x_recover_2 = x_processed_2.reshape(*front_size_2, self.hidden_size)\n",
    "        \n",
    "        return self.output(x_recover_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae7ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True) # 对数均匀分布\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-1, log=True) # 对数均匀分布\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    seq_len = trial.suggest_categorical(\"seq_len\", [30, 60, 90])\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [10,15,20,30])\n",
    "    num_layers = trial.suggest_categorical('num_layers', [1,2,3])\n",
    "    patch_size = trial.suggest_categorical('patch_size', [5,10,20])\n",
    "    in_patch_hidden_size = trial.suggest_categorical('in_patch_hidden_size', [10,15,20,30])\n",
    "    in_patch_num_layers = trial.suggest_categorical('in_patch_num_layers', [1,2,3])\n",
    "    alpha = trial.suggest_float('alpha', 1e-2, 1e-1)\n",
    "\n",
    "    # 提取数据\n",
    "    feature_columns = ['inday_chg_open','inday_chg_high','inday_chg_low','inday_chg_close','inday_chg_amplitude', 'ma_10','ma_26','ma_45','ma_90','ma_vol',]\n",
    "    label_columns = ['label_return','down_prob','middle_prob','up_prob']\n",
    "    assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX', 'AU.SHF', 'JM.DCE','RB.SHF','HC.SHF', 'I.DCE', 'M.DCE', 'CF.ZCE',]\n",
    "    assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX',]\n",
    "    feature = []\n",
    "    label = []\n",
    "\n",
    "    for asset_code in assets_list:\n",
    "        data = pd.read_csv(f'data/{asset_code}.csv')\n",
    "        feature.append(torch.tensor(data[feature_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "        label.append(torch.tensor(data[label_columns].values, dtype = torch.float32, device = 'cuda:0'))\n",
    "\n",
    "    # 加载数据\n",
    "    feature = torch.stack(feature, dim = 1)\n",
    "    label = torch.stack(label, dim = 1)\n",
    "    print(feature.shape, label.shape)\n",
    "\n",
    "    # 折叠时间步\n",
    "    feature = feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(2,3)\n",
    "    label = label[seq_len-1:]\n",
    "    print(feature.shape, label.shape)\n",
    "\n",
    "    data = RandomLoader(feature, label)\n",
    "    recorder = PredictionRecorder()\n",
    "    animator = TrainMonitor(figsize=(12,6))\n",
    "\n",
    "    result = np.zeros(shape = (10, len(assets_list), 4))\n",
    "    precision_list = []\n",
    "\n",
    "    result = []\n",
    "    for i in range(10):\n",
    "        j = 0\n",
    "        train_loader, test_loader = data(batch_size=batch_size, slice_size=[0.6, 0.1], balance=[True, False])\n",
    "\n",
    "        for x ,y in train_loader:\n",
    "            print(x.shape)\n",
    "            break\n",
    "        \n",
    "        animator.reset()\n",
    "        loss_fn = HybridLoss(alpha = 1e-2, delta = 1.3, show_loss = False) #控制损失在1：3左右\n",
    "        model = Patch_LSTM(input_size = 10,\n",
    "                           patch_size = patch_size,\n",
    "                           in_patch_hidden_size = in_patch_hidden_size,\n",
    "                           in_patch_num_layers = in_patch_num_layers,\n",
    "                           hidden_size = hidden_size,\n",
    "                           num_layers = num_layers,\n",
    "                           dropout = dropout\n",
    "                           ).to('cuda:0')\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "        train = ModelTrain(model = model,\n",
    "                    train_loader = train_loader,\n",
    "                    test_loader = test_loader,\n",
    "                    loss_fn = loss_fn,\n",
    "                    optimizer = optimizer,\n",
    "                    scheduler = scheduler,\n",
    "                    recorder = recorder,\n",
    "                    graph = animator,\n",
    "                    )\n",
    "        prediction, precision = train.epoch_train(epochs = 10, early_stop = 100)\n",
    "\n",
    "        precision_list.append(precision)\n",
    "\n",
    "\n",
    "    return np.mean(precision_list)/np.std(precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ee0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 运行优化 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 我们要最大化准确率\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=30)  # 运行30次试验\n",
    "\n",
    "    print(\"优化完成!\")\n",
    "    print(\"最佳试验的编号: \", study.best_trial.number)\n",
    "    print(\"最佳准确率: \", study.best_value)\n",
    "    print(\"最佳超参数: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835ec50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_assets = pd.DataFrame({\n",
    "#     'stage_1_prediction': np.mean(result, axis = 0)[:,0],\n",
    "#     'stage_2_prediction': np.mean(result, axis = 0)[:,2],\n",
    "\n",
    "#     'stage_1_precision': np.mean(result, axis = 0)[:,1],\n",
    "#     'stage_2_precision': np.mean(result, axis = 0)[:,3],\n",
    "\n",
    "#     'stage_1_precision_std': np.std(result, axis = 0)[:,1],\n",
    "#     'stage_2_precision_std': np.std(result, axis = 0)[:,3],\n",
    "# })\n",
    "# all_assets.index = pd.Series(assets_list)\n",
    "# for col in all_assets.columns:\n",
    "#     all_assets[col] = all_assets[col].apply(lambda x: f\"{x:.1%}\")\n",
    "\n",
    "# # 转换为Markdown\n",
    "# markdown_table = all_assets.to_markdown(index=False)\n",
    "# print(f'hidden_size: {hidden_size}, num_layers: {num_layers}, seq_len: {seq_len}')\n",
    "# print(markdown_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ee2680",
   "metadata": {},
   "source": [
    "benchmark\n",
    "| stage_1_prediction   | stage_2_prediction   | stage_1_precision   | stage_2_precision   | stage_1_precision_std   | stage_2_precision_std   |\n",
    "|:---------------------|:---------------------|:--------------------|:--------------------|:------------------------|:------------------------|\n",
    "| 21.9%                | 0.0%                 | 15.8%               | 0.0%                | 14.0%                   | 0.0%                    |\n",
    "| 0.0%                 | 0.0%                 | 0.0%                | 0.0%                | 0.0%                    | 0.0%                    |\n",
    "| 0.0%                 | 0.0%                 | 0.0%                | 0.0%                | 0.0%                    | 0.0%                    |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
