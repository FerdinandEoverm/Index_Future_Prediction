{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192725fb",
   "metadata": {},
   "source": [
    "在Baseline的单资产DLinear模型上，预测结果取得了稳定的超额准确率，但是也会出现几个问题\n",
    "\n",
    "1. 由于单资产的数据量有限，导致训练集规模不够大， 容易出现过拟合现象，导致学习不充分\n",
    "\n",
    "2. 由于模型需要在“预测结果本身是Neutral分类”和“对当前样本做出的预测不够有信心” 两种情况下都要选择放弃预测，从而导致整体预测结果会出现比真实结果更多的Neutral 分类（约90%），模型仅对10%的样本选择做出预测。\n",
    "\n",
    "为了能提高资金利用效率，防止模型在大多数情况下都在被动等待，我们可以在单资产DLinear Model的基础上，扩展资产的数量，一方面增大数据集，另一方面降低资金利用效率。\n",
    "\n",
    "在这一阶段，为了防止模型过于复杂，我们暂时还不考虑引入跨资产的截面数据关系，假定这些资产之间是完全无关的，这也符合PatchTST中通道独立的假设，"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9622e1",
   "metadata": {},
   "source": [
    "除了上证50、沪深300、中证500股指期货以外，我们选取了一些成交量好，与宏观经济有强关联的资产种类，例如黄金、原油、焦煤、螺纹钢、热卷、铁矿石、豆粕、棉花\n",
    "\n",
    "但是由于不同的期货品种共用模型参数，而资产的波动率、行为模式存在差异，导致模型的预测能力下降了。\n",
    "\n",
    "对此，我们可以有两个解决方法\n",
    "\n",
    "1. 两阶段训练模式，先训练整体，再根据目标微调参数\n",
    "\n",
    "2. 增加资产信息维度，将资产信息嵌入到价格数据中，让模型学会根据资产信息改变参数\n",
    "\n",
    "我们先实现第一种思路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c961453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('d:/future/index_future_prediction/Index_Future_Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd17e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler, Adam, AdamW\n",
    "from scipy.stats import norm, t\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07727120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.random_split import RandomSplit, CallableDataset\n",
    "from utils.back_test import BackTest\n",
    "from utils.hybrid_loss import HybridLoss\n",
    "from utils.hybrid_decoder import HybridDecoder\n",
    "from utils.prediction_recorder import PredictionRecorder\n",
    "from utils.train_animator import TrainAnimator\n",
    "from utils.model_train import ModelTrain\n",
    "from utils.get_ohlcv import GetOHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3afa819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tushare as ts\n",
    "pro = ts.pro_api('700c1d6015ad030ff20bf310c088243da030e6b79a2a1098d58d2614')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de6f84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dlinear import DLinear\n",
    "class DLinearOutput(nn.Module):\n",
    "    \"\"\"DLinear作为信息编码层，编码历史信息，然后交给HybridDecoder进行解码\"\"\"\n",
    "    def __init__(self, seq_len, pred_len, individual, enc_in, kernel_size, init_prob, dropout, **kwargs):\n",
    "        super(DLinearOutput, self).__init__(**kwargs)\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "        self.process = nn.Sequential(\n",
    "            DLinear(seq_len = seq_len, pred_len =  pred_len, individual = individual, enc_in = enc_in, kernel_size = kernel_size),\n",
    "            nn.Flatten(start_dim = 1), \n",
    "            nn.Dropout(dropout),\n",
    "            )\n",
    "\n",
    "\n",
    "        self.output = HybridDecoder(pred_len * enc_in, init_prob = init_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.process(x)\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d63754",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_list = ['IH.CFX', 'IF.CFX', 'IC.CFX', 'AU.SHF', 'FU.SHF', 'JM.DCE','RB.SHF','HC.SHF', 'I.DCE', 'M.DCE', 'CF.ZCE',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba10fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 40\n",
    "pred_len = 5\n",
    "train_ratio = 0.5\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.02\n",
    "threshold_ratio = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0f36122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_split(train_ratio, validation_ratio, test_ratio):\n",
    "    source = GetOHLCV()\n",
    "    sample_date = source.get_data('M.DCE', 5, 0.3)\n",
    "    date_column = sample_date['trade_date'].copy()\n",
    "    total_size = len(date_column)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    validation_size = int(validation_ratio * total_size)\n",
    "    test_size = int(test_ratio * total_size)\n",
    "    random_split = np.random.randint(train_size, total_size - validation_size - test_size)\n",
    "    validation_start = date_column.iloc[random_split]\n",
    "    test_start = date_column.iloc[random_split+validation_size]\n",
    "    test_end = date_column.iloc[random_split+validation_size+test_size]\n",
    "    return validation_start, test_start, test_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f0a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(assets_list, validation_start, test_start, test_end, seq_len, pred_len, threshold_ratio):\n",
    "\n",
    "    source = GetOHLCV()\n",
    "\n",
    "    train_set = None\n",
    "    validation_set = None\n",
    "    test_set = None\n",
    "    feature_column = ['log_open','log_high','log_low','log_close','log_amount']\n",
    "    label_column = ['label_return','down_prob','middle_prob','up_prob']\n",
    "    \n",
    "    for code in assets_list:\n",
    "\n",
    "        data = source.get_data(code, pred_len, threshold_ratio)\n",
    "\n",
    "        train_data = data[data['trade_date'] < validation_start].copy()\n",
    "        validation_data = data[(data['trade_date'] >= validation_start) & (data['trade_date'] < test_start)].copy()\n",
    "        test_data = data[(data['trade_date'] >= test_start) & (data['trade_date'] < test_end)].copy()\n",
    "    \n",
    "        train_feature = torch.tensor(train_data[feature_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        train_feature = train_feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(1,2)\n",
    "\n",
    "        validation_feature = torch.tensor(validation_data[feature_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        validation_feature = validation_feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(1,2)\n",
    "\n",
    "        test_feature = torch.tensor(test_data[feature_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        test_feature = test_feature.unfold(dimension = 0, size = seq_len, step = 1).transpose(1,2)\n",
    "\n",
    "\n",
    "\n",
    "        train_label = torch.tensor(train_data[label_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        train_label = train_label[seq_len-1:]\n",
    "\n",
    "        validation_label = torch.tensor(validation_data[label_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        validation_label = validation_label[seq_len-1:]\n",
    "\n",
    "        test_label = torch.tensor(test_data[label_column].values, dtype = torch.float32, device = 'cuda:0')\n",
    "        test_label = test_label[seq_len-1:]\n",
    "\n",
    "\n",
    "\n",
    "        if train_set == None:\n",
    "            train_set = CallableDataset(train_feature, train_label)\n",
    "        else:\n",
    "            train_set = train_set + CallableDataset(train_feature, train_label)\n",
    "\n",
    "        if validation_set == None:\n",
    "            validation_set = CallableDataset(validation_feature, validation_label)\n",
    "        else:\n",
    "            validation_set = validation_set + CallableDataset(validation_feature, validation_label)\n",
    "        \n",
    "        if test_set == None:\n",
    "            test_set = CallableDataset(test_feature, test_label)\n",
    "        else:\n",
    "            test_set = test_set + CallableDataset(test_feature, test_label)\n",
    "\n",
    "    return train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d02260",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_start, test_start, test_end = get_random_split(train_ratio, validation_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ceafdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, validation_set, test_set = get_data_set(assets_list, validation_start, test_start, test_end, seq_len, pred_len, threshold_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11aeded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animator data has been reset.\n"
     ]
    }
   ],
   "source": [
    "recorder = PredictionRecorder()\n",
    "animator = TrainAnimator(figsize=(12,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61af781",
   "metadata": {},
   "source": [
    "实现两阶段的训练，由于设置了学习率调度器，在一阶段结束的时候学习率已经被调度到了一个相当低的水平，就不需要重新设置学习率了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d021f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros(shape = (10, len(assets_list), 4))\n",
    "\n",
    "for i in range(10):\n",
    "    validation_start, test_start, test_end = get_random_split(train_ratio, validation_ratio, test_ratio)\n",
    "    train_set, validation_set, test_set = get_data_set(assets_list, validation_start, test_start, test_end, seq_len, pred_len, threshold_ratio)\n",
    "\n",
    "    for j in range(len(assets_list)):\n",
    "        code = assets_list[j]\n",
    "        train_set_2, validation_set_2, test_set_2 = get_data_set([code], validation_start, test_start, test_end, seq_len, pred_len, threshold_ratio)\n",
    "\n",
    "        model = DLinearOutput(seq_len = seq_len, pred_len = pred_len, individual = True, enc_in = 5, kernel_size = 21, init_prob = [0.0, 1, 0.0], dropout = 0.5).to('cuda:0')\n",
    "        loss_fn = HybridLoss(alpha = 1e-3, delta = 1)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay = 1e-1)\n",
    "        scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "        train = ModelTrain(model = model,\n",
    "                        batch_size = 100,\n",
    "                        train_set = train_set,\n",
    "                        validation_set = validation_set,\n",
    "                        test_set = test_set,\n",
    "                        loss_fn = loss_fn,\n",
    "                        optimizer = optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        recorder=recorder,\n",
    "                        graph=animator,\n",
    "                        )\n",
    "        \n",
    "        train.train_set = train_set_2\n",
    "        train.validation_set = validation_set_2\n",
    "\n",
    "        prediction, precision = train.epoch_train(epochs = 10, round = 100, early_stop = 10)\n",
    "        result[i,j,0] = prediction\n",
    "        result[i,j,1] = precision\n",
    "\n",
    "        # prediction, precision = train.epoch_train(epochs = 10, round = 100, early_stop = 10)\n",
    "        # result[i,j,2] = prediction\n",
    "        # result[i,j,3] = precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e246484",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assets = pd.DataFrame({\n",
    "    'stage_1_prediction': np.mean(result, axis = 0)[:,0],\n",
    "    'stage_2_prediction': np.mean(result, axis = 0)[:,2],\n",
    "\n",
    "    'stage_1_precision': np.mean(result, axis = 0)[:,1],\n",
    "    'stage_2_precision': np.mean(result, axis = 0)[:,3],\n",
    "\n",
    "    'stage_1_precision_std': np.std(result, axis = 0)[:,1],\n",
    "    'stage_2_precision_std': np.std(result, axis = 0)[:,3],\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e33ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_assets_styled = all_assets.style.format({\n",
    "    'stage_1_prediction': '{:.1%}',\n",
    "    'stage_1_precision': '{:.1%}',\n",
    "    'stage_1_precision_std': '{:.1%}',\n",
    "    'stage_2_prediction': '{:.1%}',\n",
    "    'stage_2_precision': '{:.1%}',\n",
    "    'stage_2_precision_std': '{:.1%}',\n",
    "})\n",
    "all_assets_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e2245",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(all_assets.to_markdown(index=False)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
