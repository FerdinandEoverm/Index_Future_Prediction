{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894c4602",
   "metadata": {},
   "source": [
    "现在，我们可以搭建同时建模截面关系和时序关系的复合transformer架构了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c39d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('d:/future/Index_Future_Prediction')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import optuna\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler, Adam, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from utils import *\n",
    "from modules import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f96dc34",
   "metadata": {},
   "source": [
    "我们需要重新定义encoder层，现在encoder层需要两次注意力关注，一次在时序上，关注本资产的前后序列；另一次在截面上，关注同时期的其他资产。\n",
    "\n",
    "为什么不在整个回望窗口内进行一个大的注意力机制呢？因为复杂度问题，假设资产数是 M 时间步是 N 全局注意力的开销是 O (MN)^2\n",
    "\n",
    "而时序和截面相当于是进行了两次稀疏注意力，且都是是具有比较强的可解释性的：分析一个时间点的信息，看一看前后和同时间点的其他资产，肯定比看不同时间的不同资产更重要吧？\n",
    "\n",
    "那如果确实有滞后信息需要传递呢？假设真的存在某种滞后关系，例如资产A的价格波动会在10天之后传导到B，这种机制也会被多层注意力捕获，因为我们的encoder层也是多层重叠的。第一次A的资产波动会传导到10天后的A，第二次则会从10天后的A传到到10天后的B，从而完成这种滞后效应的建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3d2f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.attention import MultiHeadAttention\n",
    "from modules.addnorm import AddNorm\n",
    "from modules.ffn import PositionWiseFFN\n",
    "\n",
    "class PanelEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Panel data transformer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_head, num_ffn_hidden, dropout):\n",
    "        super().__init__()\n",
    "        # 纵向时间序列注意力；\n",
    "        self.time_series_attention = MultiHeadAttention(d_model, num_head)\n",
    "        # 横向截面注意力；\n",
    "        self.cross_section_attention = MultiHeadAttention(d_model, num_head)\n",
    "        # addnorm 层\n",
    "        self.addnorm = AddNorm(normalized_shape=(d_model,), dropout=dropout)\n",
    "        # 通过ffn 整合信息\n",
    "        self.ffn = PositionWiseFFN(d_model, num_ffn_hidden, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        imput and output size: (batch_size, num_assets, seq_len or num_patch, d_model)\n",
    "        \"\"\"\n",
    "        # 注意力机制会自动展平前面的层，把倒数第二层作为注意力的范围。对于时序注意力，倒数第二维度应该是时间步长度\n",
    "        ts_out = self.time_series_attention(x,x,x, mask)\n",
    "        ts_out = self.addnorm(x, ts_out)\n",
    "\n",
    "        ts_out = ts_out.permute(0,2,1,3)# 这里交换num_assets 和 seq_len 来把资产数交换到倒数第二个维度上，让注意力关注截面\n",
    "        cs_out = self.cross_section_attention(ts_out,ts_out,ts_out, mask)\n",
    "        cs_out = cs_out.permute(0,2,1,3)# 记得交换回来\n",
    "        cs_out = self.addnorm(x, cs_out)\n",
    "        \n",
    "        # 最后通过ffn 整理当前时间步内部的信息\n",
    "        ffn_out = self.ffn(cs_out)\n",
    "        ffn_out = self.addnorm(x, ffn_out)\n",
    "        return ffn_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51a91e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPanelEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    多层PanelEncoder，由多个PanelEncoderBlock堆叠而成\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, d_model, num_head, num_ffn_hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([PanelEncoderBlock(d_model = d_model, num_head = num_head,num_ffn_hidden = num_ffn_hidden,dropout = dropout,)for _ in range(num_layer)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee6dd24",
   "metadata": {},
   "source": [
    "相比于预测单个资产，现在我们要预测一组资产"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa840b9b",
   "metadata": {},
   "source": [
    "为什么不选用经典transformer的加性位置编码呢？原因有两个，加性位置编码的优点是节省维度，但缺点是模型需要首先学会从汇总的信息中分离位置信息和原始信息。\n",
    "\n",
    "但是我们的时序预测维度并不高，不需要节省维度，反而我们的数据量是不足的，没必要浪费额外的成本来训练这个，因此采用concate的位置编码更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80196136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Time2Vec时序编码，以concat形式扩展位置编码。\n",
    "    原始输入维度: (*, seq_len, d_model)\n",
    "    输出维度: (*, seq_len, d_model + dim_embedding)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_embedding):\n",
    "        super().__init__()\n",
    "        self.dim_embedding = dim_embedding\n",
    "        \n",
    "        # Time2Vec 的可学习参数\n",
    "        self.w = nn.Parameter(torch.empty(1, self.dim_embedding), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.empty(1, self.dim_embedding), requires_grad=True)\n",
    "        # 初始化参数\n",
    "        nn.init.uniform_(self.w, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.b, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        输入形状为 (*, seq_len, d_model)\n",
    "        输出形状为 (*, seq_len, d_model + dim_embedding)\n",
    "        \"\"\"\n",
    "        # 保存初始形状\n",
    "        original_shape = x.shape # (*, seq_len, feature_dim)\n",
    "        seq_len = original_shape[-2]\n",
    "        batch_dims = original_shape[:-2]\n",
    "        \n",
    "        # 相对时间序号： [0, 1, 2, ..., seq_len-1]\n",
    "        tau = torch.arange(seq_len, dtype=torch.float, device=x.device).unsqueeze(-1)\n",
    "\n",
    "        # 计算时间嵌入\n",
    "        time_embedding = tau @ self.w + self.b\n",
    "        \n",
    "        linear_part = time_embedding[:, :1] # 线性部分\n",
    "        periodic_part = torch.sin(time_embedding[:, 1:]) # 周期性部分\n",
    "\n",
    "        time_embedding = torch.cat([linear_part, periodic_part], dim=-1)\n",
    "\n",
    "        # 把编码广播到所有维度\n",
    "        target_shape = batch_dims + (seq_len, self.dim_embedding)\n",
    "        time_embedding = time_embedding.expand(target_shape)\n",
    "\n",
    "        # 拼接\n",
    "        output = torch.cat([x, time_embedding], dim=-1)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba9b57f",
   "metadata": {},
   "source": [
    "Panel_Transformer 的主体部分，在不同任务之间通用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9e4cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PanelTransformerBackbone(nn.Module):\n",
    "    def __init__(self, dim_patch_feature, dim_projection, dim_temporal_embedding, dim_assets_embedding, num_bass_assets, num_head, num_layer, dropout):\n",
    "        super().__init__()\n",
    "        dim_encoder_input = dim_projection +  dim_temporal_embedding + dim_assets_embedding\n",
    "        self.projection = nn.Linear(dim_patch_feature, dim_projection)\n",
    "        self.assets_embedding = AssetsEmbedding(num_base_assets = num_bass_assets, embedding_dim = dim_assets_embedding, target_ratio = 0.2, freeze = True)\n",
    "        self.temporal_embedding = TemporalEmbedding(dim_embedding = dim_temporal_embedding)\n",
    "        self.panel_encoder = MultiLayerPanelEncoder(num_layer = num_layer, d_model = dim_encoder_input, num_head = num_head, num_ffn_hidden = dim_encoder_input * 2, dropout = dropout)\n",
    "\n",
    "    def forward(self, x, weights):\n",
    "        x = self.projection(x)\n",
    "        x = self.temporal_embedding(x)\n",
    "        x = self.assets_embedding(x, weights)\n",
    "        x = self.panel_encoder(x)\n",
    "        return x\n",
    "\n",
    "model = PanelTransformerBackbone(dim_patch_feature = 120, dim_projection = 128, dim_temporal_embedding = 38, dim_assets_embedding = 10, num_bass_assets = 53, num_head = 8, num_layer = 3 , dropout = 0.5)\n",
    "model.assets_embedding.load_state_dict(torch.load('params/assets_embedding.params'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15e633e",
   "metadata": {},
   "source": [
    "预训练层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "153d29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainPanelTransformer(nn.Module):\n",
    "    \"\"\"Panel Time Series Transformer\"\"\"\n",
    "    def __init__(self, dim_raw_feature, patch_size, stride, mask_expand_size, seq_len, dim_projection, dim_temporal_embedding, dim_assets_embedding, num_bass_assets, num_head, num_layer, dropout):\n",
    "        super().__init__()\n",
    "        # 模型参数\n",
    "        self.device = 'cuda:0'\n",
    "        self.input_size = dim_raw_feature\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.mask_expand_size = mask_expand_size\n",
    "        self.num_patch = int(np.floor((seq_len - patch_size) / stride) + 1)\n",
    "\n",
    "        self.dim_projection = dim_projection\n",
    "\n",
    "        dim_encoder_input = dim_projection +  dim_temporal_embedding + dim_assets_embedding\n",
    "\n",
    "        # 前置层\n",
    "        self.patch = TimeSeriesPatcher(patch_size, stride)\n",
    "\n",
    "        # 编码层\n",
    "        self.encoder = PanelTransformerBackbone(dim_patch_feature = dim_raw_feature * patch_size,\n",
    "                                                  dim_projection = dim_projection,\n",
    "                                                  dim_temporal_embedding = dim_temporal_embedding,\n",
    "                                                  dim_assets_embedding = dim_assets_embedding,\n",
    "                                                  num_bass_assets = num_bass_assets,\n",
    "                                                  num_head = num_head,\n",
    "                                                  num_layer = num_layer,\n",
    "                                                  dropout = dropout)\n",
    "\n",
    "        # 预训练输出层\n",
    "        self.reconstruction = nn.Linear(dim_encoder_input, dim_raw_feature * patch_size)\n",
    "\n",
    "    \n",
    "    def _mask(self, x):\n",
    "        \"\"\"\n",
    "        对于批次中的每个样本，随机选择一个 patch 索引。\n",
    "        然后，该样本的所有 assets 中对应于该索引的 patch (包括其扩展区域) 都将被置为 0。\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        batch_size = x.shape[0]\n",
    "        num_assets = x.shape[1] \n",
    "\n",
    "        # 为批次中的每个样本随机选择一个 patch 索引进行 mask\n",
    "        masked_patch_indices = torch.randint(0, self.num_patch, (batch_size, 1), device=device)\n",
    "\n",
    "        # 在每个样本被选中的位置shi 1\n",
    "        one_hot_mask = torch.zeros((batch_size, self.num_patch), device=device)\n",
    "        one_hot_mask.scatter_(1, masked_patch_indices, 1)\n",
    "        # 最终 target_mask 的形状为: (batch_size, num_assets, num_patch)\n",
    "        target_mask = one_hot_mask.unsqueeze(1).expand(-1, num_assets, -1).bool()\n",
    "\n",
    "        # 需要扩展掩蔽，避免模型从相邻 patch 偷看重叠部分\n",
    "        # target_mask_float 的形状: (batch_size * num_assets, 1, num_patch)\n",
    "        target_mask_float = target_mask.float().view(batch_size * num_assets, 1, self.num_patch)\n",
    "        \n",
    "        # 使用一维卷积来扩展掩码区域\n",
    "        kernel = torch.ones(1, 1, 2 * self.mask_expand_size + 1, device=device)\n",
    "        expanded_mask_float = F.conv1d(target_mask_float, kernel, padding=self.mask_expand_size)\n",
    "        input_mask = (expanded_mask_float > 0).squeeze(1).view(batch_size, num_assets, self.num_patch)\n",
    "        \n",
    "        # 扩展掩码的最后一个维度\n",
    "        reshape_mask = input_mask.unsqueeze(-1)\n",
    "        masked_x = torch.where(reshape_mask, 0.0, x)\n",
    "        \n",
    "        return masked_x, target_mask\n",
    "    \n",
    "    def forward(self, x, weights):\n",
    "        x_patched = self.patch(x)        \n",
    "        masked_x, target_mask = self._mask(x_patched)\n",
    "        enc_out = self.encoder(masked_x, weights)\n",
    "        x_pre_reconstruction = enc_out[target_mask]\n",
    "        x_reconstructed = self.reconstruction(x_pre_reconstruction)\n",
    "        x_target = x_patched[target_mask]\n",
    "\n",
    "        return x_reconstructed, x_target\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81a56de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_list = [\n",
    "    # 股指期货\n",
    "    'IH.CFX', 'IF.CFX', 'IC.CFX',\n",
    "    # 国债期货\n",
    "    'TS.CFX', 'TF.CFX', 'T.CFX', 'TL1.CFX',\n",
    "    # 黑色金属产业链\n",
    "    'I.DCE', 'JM.DCE', 'RB.SHF', 'HC.SHF', 'SS.SHF', 'SF.ZCE', 'SM.ZCE',\n",
    "    # 有色金属\n",
    "    'CU.SHF', 'AL.SHF', 'ZN.SHF', 'NI.SHF',\n",
    "    # 贵金属\n",
    "    'AU.SHF', 'AG.SHF',\n",
    "    # 能源化工\n",
    "    'FU.SHF', 'LU.INE', 'BU.SHF', 'PG.DCE', 'TA.ZCE', 'EG.DCE', 'PF.ZCE', \n",
    "    'L.DCE', 'PP.DCE', 'V.DCE', 'EB.DCE', 'MA.ZCE', 'UR.ZCE', 'RU.SHF',\n",
    "    # 农产品\n",
    "    'A.DCE', 'B.DCE', 'M.DCE', 'RM.ZCE', 'Y.DCE', 'OI.ZCE', 'P.DCE', 'PK.ZCE',\n",
    "    'C.DCE', 'CS.DCE', 'CF.ZCE', 'SR.ZCE', 'CJ.ZCE', 'AP.ZCE', 'SP.SHF', \n",
    "    'JD.DCE', 'LH.DCE',\n",
    "    # 建材\n",
    "    'FG.ZCE', 'SA.ZCE'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1e06b",
   "metadata": {},
   "source": [
    "经过多次超参数调优，我发现了非常有趣的一点：\n",
    "\n",
    "在指定的patch size 和 stride 之下，可以得到一个防止模型偷看的最低expand size，即当patch size = 8 stride = 4 的时候，理论上只需要前后屏蔽一个patch 就可以防止偷看。\n",
    "\n",
    "此时如果设定mask expand size = 1, 可以发现模型会很快收敛，但是收敛到一个比较大的值就不再下降了（约0.2）；\n",
    "\n",
    "反而如果设定mask expand size = 2 或更高，理论上来说这样是提高了预测的难度，模型能达到的最终loss应该更高；但是在实际中我发现模型可以一直学习，且最终能达到0.13或更低；\n",
    "\n",
    "我猜测这里很有可能是遇到了插值现象，前后只屏蔽一个patch 数据仍然是相邻的，模型可以通过简单插值就估算出中间被屏蔽的patch中的信息，然后就陷入在这样的局部最优里，不会进一步学习复杂的特征表征\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b10ac315",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_raw_feature = 10\n",
    "patch_size = 10\n",
    "stride = 5\n",
    "mask_expand_size = 3\n",
    "seq_len = 120\n",
    "dim_projection = 102\n",
    "dim_temporal_embedding = 16\n",
    "dim_assets_embedding = 10\n",
    "num_bass_assets = 53\n",
    "num_head = 8\n",
    "num_layer = 5\n",
    "dropout = 0.3\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 0\n",
    "gamma = 1\n",
    "\n",
    "test_size = 128\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a96c3b",
   "metadata": {},
   "source": [
    "在数据处理层，有很大不同；因为我们现在一次输入的是一组资产，不能再以某一个资产的涨跌来进行训练均衡了。因此我们可以直接调用原生的 dataset 和 dataloader\n",
    "\n",
    "在variable_length_input.ipynb 里，我们预先处理好了数据，这里只需要加载就可以了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e5c75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "loaders = []\n",
    "for i in [20110901, 20160901, 20210901]:\n",
    "\n",
    "    feature = torch.load(f'data/feature_{i}').to('cuda:0')\n",
    "    label = torch.load(f'data/label_{i}').to('cuda:0')\n",
    "    weights = torch.load(f'data/weights_{i}').to('cuda:0')\n",
    "\n",
    "    train_set = TensorDataset(feature[:-256], weights[:-256], label[:-256])\n",
    "    train_loader = DataLoader(train_set, batch_size, True)\n",
    "    loaders.append(train_loader)\n",
    "\n",
    "\n",
    "feature = torch.load('data/feature_20210901').to('cuda:0')\n",
    "label = torch.load('data/label_20210901').to('cuda:0')\n",
    "weights = torch.load('data/weights_20210901').to('cuda:0')\n",
    "test_set = TensorDataset(feature[-128:], weights[-128:], label[-128:])\n",
    "test_loader = DataLoader(test_set, batch_size, True)\n",
    "\n",
    "# test_loader 只需要最新的就行了，因为是一样的，重复test没意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05833b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 9/200 [00:39<14:06,  4.43s/it]"
     ]
    }
   ],
   "source": [
    "# 预训练\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "model = PretrainPanelTransformer(dim_raw_feature, patch_size, stride, mask_expand_size, seq_len, dim_projection, dim_temporal_embedding, dim_assets_embedding, num_bass_assets, num_head, num_layer, dropout).to('cuda:0')\n",
    "model.encoder.assets_embedding.load_state_dict(torch.load('params/assets_embedding.params'))\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "\n",
    "\n",
    "def epoch():\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for loader in loaders:\n",
    "        for batch_x, batch_weights, batch_y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            x_reconstructed, x_target = model(batch_x, batch_weights)\n",
    "            loss = loss_fn(x_reconstructed, x_target)\n",
    "            train_losses.append(loss.item()) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    test_losses = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_weights, batch_y in test_loader:\n",
    "            x_reconstructed, x_target = model(batch_x, batch_weights)\n",
    "            loss = loss_fn(x_reconstructed, x_target)\n",
    "            test_losses.append(loss.item()) \n",
    "    return np.mean(train_losses), np.mean(test_losses)\n",
    "\n",
    "def train(epochs = 30):\n",
    "    current = 0\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        train_loss, test_loss = epoch()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        scheduler.step()\n",
    "        if i > 50:\n",
    "            if i % 10 == 0:\n",
    "                if np.mean(test_losses[-10:]) > np.mean(test_losses[-20:-10]):\n",
    "                    current = i\n",
    "                    break\n",
    "    plt.plot(range(i+1), train_losses)\n",
    "    plt.plot(range(i+1), test_losses)\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(test_losses[-5:])\n",
    "\n",
    "final_loss = train(epochs)\n",
    "print(final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1dceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), 'params/panel_tf_backbone.params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b4f01",
   "metadata": {},
   "source": [
    "0.12697334289550782"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
